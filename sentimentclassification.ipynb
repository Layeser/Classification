{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1530b00e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-08T13:34:24.168670Z",
     "iopub.status.busy": "2025-05-08T13:34:24.168365Z",
     "iopub.status.idle": "2025-05-08T13:35:07.896398Z",
     "shell.execute_reply": "2025-05-08T13:35:07.895616Z"
    },
    "papermill": {
     "duration": 43.737269,
     "end_time": "2025-05-08T13:35:07.900804",
     "exception": false,
     "start_time": "2025-05-08T13:34:24.163535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno -3] Temporary\n",
      "[nltk_data]     failure in name resolution>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "nltk.download('punkt')\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import re\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb96b12d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T13:35:07.908447Z",
     "iopub.status.busy": "2025-05-08T13:35:07.908087Z",
     "iopub.status.idle": "2025-05-08T13:35:07.917567Z",
     "shell.execute_reply": "2025-05-08T13:35:07.917029Z"
    },
    "papermill": {
     "duration": 0.014304,
     "end_time": "2025-05-08T13:35:07.918543",
     "exception": false,
     "start_time": "2025-05-08T13:35:07.904239",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, corpus, min_freq=2):\n",
    "        self.corpus = corpus\n",
    "        self.tokens = nltk.word_tokenize(corpus.lower())\n",
    "        \n",
    "        self.freq = Counter(self.tokens)\n",
    "        \n",
    "        self.vocab = {\n",
    "            '[PAD]': 0,\n",
    "            '[UNK]': 1,\n",
    "            '[CLS]': 2,\n",
    "            '[SEP]': 3,\n",
    "        }\n",
    "        \n",
    "        idx = 4\n",
    "        for word in sorted(self.freq):\n",
    "            if self.freq[word] >= min_freq:\n",
    "                self.vocab[word] = idx\n",
    "                idx += 1\n",
    "\n",
    "        self.id_to_word = {idx: word for word, idx in self.vocab.items()}\n",
    "    \n",
    "    def tokenize(self, sentence):\n",
    "        return nltk.word_tokenize(sentence.lower())\n",
    "    \n",
    "    def batch_tokenize(self, batch_sentence):\n",
    "        return [self.tokenize(sentence) for sentence in tqdm(batch_sentence)]\n",
    "    \n",
    "    def encode(self, tokens):\n",
    "        return [self.vocab.get(word, self.vocab['[UNK]']) for word in tokens]\n",
    "    \n",
    "    def batch_encode(self, batch_tokens):\n",
    "        return [self.encode(tokens) for tokens in tqdm(batch_tokens)]\n",
    "    \n",
    "    def decode(self, sequence):\n",
    "        return ' '.join(self.id_to_word.get(idx, '[UNK]') for idx in sequence)\n",
    "    \n",
    "    def batch_decode(self, batch_sequences):\n",
    "        return [self.decode(seq) for seq in tqdm(batch_sequences)]\n",
    "    \n",
    "    def prepare_inputs(self, tokens, max_seq):\n",
    "        cls_token = [self.vocab['[CLS]']]\n",
    "        sep_token = [self.vocab['[SEP]']]\n",
    "        \n",
    "        encoded = self.encode(tokens)\n",
    "        max_tokens = max_seq - 2\n",
    "        truncated = encoded[:max_tokens]\n",
    "        \n",
    "        full_seq = cls_token + truncated + sep_token\n",
    "        padding_length = max_seq - len(full_seq)\n",
    "        padded_seq = full_seq + [self.vocab['[PAD]']] * padding_length\n",
    "        \n",
    "        return padded_seq\n",
    "    \n",
    "    def batch_prepare_inputs(self, batch_tokens, max_seq):\n",
    "        return [self.prepare_inputs(tokens, max_seq) for tokens in tqdm(batch_tokens)]\n",
    "\n",
    "    def decode_on_inputs(self, sequence, skip_special_tokens=True):\n",
    "        words = []\n",
    "        for idx in sequence:\n",
    "            word = self.id_to_word.get(idx, '[UNK]')\n",
    "            if skip_special_tokens and word in {'[PAD]', '[CLS]', '[SEP]'}:\n",
    "                continue\n",
    "            words.append(word)\n",
    "        return ' '.join(words)\n",
    "\n",
    "    def batch_decode_on_inputs(self, batch_sequences, skip_special_tokens=True):\n",
    "        return [self.decode_on_inputs(seq, skip_special_tokens) for seq in tqdm(batch_sequences)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "238797c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T13:35:07.925383Z",
     "iopub.status.busy": "2025-05-08T13:35:07.925191Z",
     "iopub.status.idle": "2025-05-08T13:35:07.928984Z",
     "shell.execute_reply": "2025-05-08T13:35:07.928457Z"
    },
    "papermill": {
     "duration": 0.008286,
     "end_time": "2025-05-08T13:35:07.930008",
     "exception": false,
     "start_time": "2025-05-08T13:35:07.921722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    df = df = pd.read_csv(path, encoding='latin-1', header=None).drop([0, 1], axis=1)\n",
    "    df.columns = ['sentiment', 'text']\n",
    "    sentiment2id = {'Positive': 0, 'Neutral': 1, 'Negative': 2, 'Irrelevant': 3}\n",
    "    df['sentiment'] = df['sentiment'].map(sentiment2id)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c33152d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T13:35:07.936881Z",
     "iopub.status.busy": "2025-05-08T13:35:07.936666Z",
     "iopub.status.idle": "2025-05-08T13:35:07.941417Z",
     "shell.execute_reply": "2025-05-08T13:35:07.940903Z"
    },
    "papermill": {
     "duration": 0.009359,
     "end_time": "2025-05-08T13:35:07.942478",
     "exception": false,
     "start_time": "2025-05-08T13:35:07.933119",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean raw text for tokenization:\n",
    "      - Lowercase all text\n",
    "      - Remove URLs (http, https, www) and t.co short links\n",
    "      - Collapse long runs of punctuation\n",
    "      - Strip out non‑ASCII (e.g. emojis)\n",
    "      - Remove stray slashes and repeated slashes\n",
    "      - Normalize whitespace\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Remove URLs (http://, https://, www.) and Twitter t.co links\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+|//t\\.co/\\S+', ' ', text)\n",
    "\n",
    "    # 3. Collapse runs of dots (4+ → '...') and other punctuation (4+ → single char)\n",
    "    text = re.sub(r'\\.{4,}', '...', text)\n",
    "    text = re.sub(r'([!?])\\1{1,}', r'\\1', text)\n",
    "\n",
    "    # 4. Remove stray slashes (2+ → single '/')\n",
    "    text = re.sub(r'/{2,}', '/', text)\n",
    "\n",
    "    # 5. Strip non‑ASCII (emojis, fancy quotes, etc.)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "\n",
    "    # 6. Remove any remaining standalone punctuation tokens longer than 3 chars\n",
    "    text = re.sub(r'([^\\w\\s]){4,}', r'\\1', text)\n",
    "\n",
    "    # 7. Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "729bb4cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T13:35:07.949334Z",
     "iopub.status.busy": "2025-05-08T13:35:07.949135Z",
     "iopub.status.idle": "2025-05-08T13:35:08.243303Z",
     "shell.execute_reply": "2025-05-08T13:35:08.242438Z"
    },
    "papermill": {
     "duration": 0.299234,
     "end_time": "2025-05-08T13:35:08.244797",
     "exception": false,
     "start_time": "2025-05-08T13:35:07.945563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_train = \"/kaggle/input/twitter-entity-sentiment-analysis/twitter_training.csv\"\n",
    "path_val = \"/kaggle/input/twitter-entity-sentiment-analysis/twitter_validation.csv\"\n",
    "\n",
    "df_train = load_data(path_train)\n",
    "df_val = load_data(path_val)\n",
    "df_train = df_train.dropna(subset=['text'])\n",
    "df_val = df_val.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "414e30af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T13:35:08.252811Z",
     "iopub.status.busy": "2025-05-08T13:35:08.252130Z",
     "iopub.status.idle": "2025-05-08T13:35:09.989403Z",
     "shell.execute_reply": "2025-05-08T13:35:09.988641Z"
    },
    "papermill": {
     "duration": 1.742786,
     "end_time": "2025-05-08T13:35:09.990950",
     "exception": false,
     "start_time": "2025-05-08T13:35:08.248164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train['clean_text'] = df_train['text'].apply(clean_text)\n",
    "df_val['clean_text'] = df_val['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43cf2680",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T13:35:09.998486Z",
     "iopub.status.busy": "2025-05-08T13:35:09.998043Z",
     "iopub.status.idle": "2025-05-08T13:35:10.005194Z",
     "shell.execute_reply": "2025-05-08T13:35:10.004542Z"
    },
    "papermill": {
     "duration": 0.011941,
     "end_time": "2025-05-08T13:35:10.006292",
     "exception": false,
     "start_time": "2025-05-08T13:35:09.994351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('im getting on borderlands and i will murder you all ,', 0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['clean_text'][0], df_train['sentiment'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62f69a81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T13:35:10.013617Z",
     "iopub.status.busy": "2025-05-08T13:35:10.013065Z",
     "iopub.status.idle": "2025-05-08T13:35:19.936623Z",
     "shell.execute_reply": "2025-05-08T13:35:19.936031Z"
    },
    "papermill": {
     "duration": 9.928491,
     "end_time": "2025-05-08T13:35:19.937989",
     "exception": false,
     "start_time": "2025-05-08T13:35:10.009498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "texts = \" \".join(df_train['text'].values)\n",
    "corpus = clean_text(texts)\n",
    "tokenizer = Tokenizer(\" \".join(df_train['clean_text'].values), min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d44b4fce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T13:35:19.945823Z",
     "iopub.status.busy": "2025-05-08T13:35:19.945393Z",
     "iopub.status.idle": "2025-05-08T13:35:19.950056Z",
     "shell.execute_reply": "2025-05-08T13:35:19.949418Z"
    },
    "papermill": {
     "duration": 0.009584,
     "end_time": "2025-05-08T13:35:19.951151",
     "exception": false,
     "start_time": "2025-05-08T13:35:19.941567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28406"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7250d0ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T13:35:19.958225Z",
     "iopub.status.busy": "2025-05-08T13:35:19.957828Z",
     "iopub.status.idle": "2025-05-08T13:35:31.499076Z",
     "shell.execute_reply": "2025-05-08T13:35:31.498326Z"
    },
    "papermill": {
     "duration": 11.545964,
     "end_time": "2025-05-08T13:35:31.500184",
     "exception": false,
     "start_time": "2025-05-08T13:35:19.954220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73996/73996 [00:10<00:00, 6995.26it/s]\n",
      "100%|██████████| 73996/73996 [00:00<00:00, 101918.60it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 5985.47it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 120139.32it/s]\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = 64\n",
    "train_inputs = tokenizer.batch_prepare_inputs(\n",
    "    tokenizer.batch_tokenize(df_train['clean_text'].values), max_seq=max_seq_len\n",
    ")\n",
    "val_inputs = tokenizer.batch_prepare_inputs(\n",
    "    tokenizer.batch_tokenize(df_val['clean_text'].values), max_seq=max_seq_len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b8d69f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T13:35:31.516374Z",
     "iopub.status.busy": "2025-05-08T13:35:31.516144Z",
     "iopub.status.idle": "2025-05-08T13:35:32.298847Z",
     "shell.execute_reply": "2025-05-08T13:35:32.298054Z"
    },
    "papermill": {
     "duration": 0.791999,
     "end_time": "2025-05-08T13:35:32.300256",
     "exception": false,
     "start_time": "2025-05-08T13:35:31.508257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_inputs = torch.tensor(train_inputs)\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "train_labels = torch.tensor(df_train['sentiment'].values, dtype=torch.long)\n",
    "val_labels = torch.tensor(df_val['sentiment'].values, dtype=torch.long)\n",
    "\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_labels)\n",
    "val_data = TensorDataset(val_inputs, val_labels)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9b5c850",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T13:35:32.318860Z",
     "iopub.status.busy": "2025-05-08T13:35:32.318296Z",
     "iopub.status.idle": "2025-05-08T13:35:32.349588Z",
     "shell.execute_reply": "2025-05-08T13:35:32.348913Z"
    },
    "papermill": {
     "duration": 0.040817,
     "end_time": "2025-05-08T13:35:32.350679",
     "exception": false,
     "start_time": "2025-05-08T13:35:32.309862",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: [CLS] i mentioned on facebook that i was struggling for motivation to go for a run the other day , which has been translated by tom s great auntie as hayley can t get out of bed and told to his grandma , who now thinks i m a lazy , terrible person [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Label: 3\n"
     ]
    }
   ],
   "source": [
    "for text, label in val_loader:\n",
    "    print(f\"Text: {tokenizer.decode(text[0].tolist())}\\nLabel: {label[0]}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5aaf63e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T13:35:32.366375Z",
     "iopub.status.busy": "2025-05-08T13:35:32.366180Z",
     "iopub.status.idle": "2025-05-08T13:35:32.371787Z",
     "shell.execute_reply": "2025-05-08T13:35:32.370955Z"
    },
    "papermill": {
     "duration": 0.014772,
     "end_time": "2025-05-08T13:35:32.372959",
     "exception": false,
     "start_time": "2025-05-08T13:35:32.358187",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: i mentioned on facebook that i was struggling for motivation to go for a run the other day , which has been translated by tom s great auntie as hayley can t get out of bed and told to his grandma , who now thinks i m a lazy , terrible person\n",
      "Label: 3\n"
     ]
    }
   ],
   "source": [
    "for text, label in val_loader:\n",
    "    print(f\"Text: {tokenizer.decode_on_inputs(text[0].tolist())}\\nLabel: {label[0]}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25c5c6b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T13:35:32.388659Z",
     "iopub.status.busy": "2025-05-08T13:35:32.388313Z",
     "iopub.status.idle": "2025-05-08T13:35:32.740739Z",
     "shell.execute_reply": "2025-05-08T13:35:32.739912Z"
    },
    "papermill": {
     "duration": 0.361928,
     "end_time": "2025-05-08T13:35:32.742269",
     "exception": false,
     "start_time": "2025-05-08T13:35:32.380341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SentimentAnalysis(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, num_classes, dropout_rate=0.5):\n",
    "        super(SentimentAnalysis, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, \n",
    "                            batch_first=True, dropout=0.2 if num_layers > 1 else 0.)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "        hidden_last = hidden[-1] \n",
    "        hidden_last = self.dropout(hidden_last)\n",
    "        out = self.fc(hidden_last)\n",
    "        return out\n",
    "\n",
    "\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SentimentAnalysis(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=128,\n",
    "    hidden_dim=256,\n",
    "    num_layers=2,\n",
    "    num_classes=4,\n",
    "    dropout_rate=0.5\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8c629a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T13:35:32.758781Z",
     "iopub.status.busy": "2025-05-08T13:35:32.758245Z",
     "iopub.status.idle": "2025-05-08T13:35:32.764239Z",
     "shell.execute_reply": "2025-05-08T13:35:32.763533Z"
    },
    "papermill": {
     "duration": 0.015272,
     "end_time": "2025-05-08T13:35:32.765384",
     "exception": false,
     "start_time": "2025-05-08T13:35:32.750112",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=3, factor=0.5)  # Added\n",
    "\n",
    "best_val_acc = 0\n",
    "early_stop_counter = 0\n",
    "patience = 5  # Added early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4046ad0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T13:35:32.781200Z",
     "iopub.status.busy": "2025-05-08T13:35:32.780967Z",
     "iopub.status.idle": "2025-05-08T13:41:30.412587Z",
     "shell.execute_reply": "2025-05-08T13:41:30.411780Z"
    },
    "papermill": {
     "duration": 357.640914,
     "end_time": "2025-05-08T13:41:30.413884",
     "exception": false,
     "start_time": "2025-05-08T13:35:32.772970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 01/30: 100%|██████████| 1157/1157 [00:18<00:00, 62.25it/s, loss=1.37, acc=0.306]\n",
      "[Val] Epoch: 01/30: 100%|██████████| 16/16 [00:00<00:00, 155.26it/s, loss=1.36, acc=0.328]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01: Train Loss: 1.3653 Acc: 0.3064 | Val Loss: 1.3610 Acc: 0.3280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 02/30: 100%|██████████| 1157/1157 [00:17<00:00, 65.29it/s, loss=1.34, acc=0.348]\n",
      "[Val] Epoch: 02/30: 100%|██████████| 16/16 [00:00<00:00, 166.73it/s, loss=1.24, acc=0.443]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 02: Train Loss: 1.3440 Acc: 0.3479 | Val Loss: 1.2398 Acc: 0.4430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 03/30: 100%|██████████| 1157/1157 [00:17<00:00, 65.29it/s, loss=1.08, acc=0.542]\n",
      "[Val] Epoch: 03/30: 100%|██████████| 16/16 [00:00<00:00, 165.54it/s, loss=0.789, acc=0.701]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 03: Train Loss: 1.0829 Acc: 0.5415 | Val Loss: 0.7888 Acc: 0.7010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 04/30: 100%|██████████| 1157/1157 [00:17<00:00, 65.35it/s, loss=0.714, acc=0.728]\n",
      "[Val] Epoch: 04/30: 100%|██████████| 16/16 [00:00<00:00, 166.52it/s, loss=0.639, acc=0.825]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 04: Train Loss: 0.7140 Acc: 0.7279 | Val Loss: 0.6387 Acc: 0.8250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 05/30: 100%|██████████| 1157/1157 [00:17<00:00, 65.17it/s, loss=0.418, acc=0.851]\n",
      "[Val] Epoch: 05/30: 100%|██████████| 16/16 [00:00<00:00, 168.59it/s, loss=0.586, acc=0.879]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 05: Train Loss: 0.4181 Acc: 0.8506 | Val Loss: 0.5856 Acc: 0.8790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 06/30: 100%|██████████| 1157/1157 [00:17<00:00, 65.24it/s, loss=0.261, acc=0.908]\n",
      "[Val] Epoch: 06/30: 100%|██████████| 16/16 [00:00<00:00, 170.86it/s, loss=0.451, acc=0.905]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 06: Train Loss: 0.2609 Acc: 0.9080 | Val Loss: 0.4514 Acc: 0.9050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 07/30: 100%|██████████| 1157/1157 [00:17<00:00, 65.28it/s, loss=0.185, acc=0.935]\n",
      "[Val] Epoch: 07/30: 100%|██████████| 16/16 [00:00<00:00, 165.39it/s, loss=0.522, acc=0.902]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 07: Train Loss: 0.1848 Acc: 0.9345 | Val Loss: 0.5221 Acc: 0.9020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 08/30: 100%|██████████| 1157/1157 [00:17<00:00, 65.37it/s, loss=0.144, acc=0.949]\n",
      "[Val] Epoch: 08/30: 100%|██████████| 16/16 [00:00<00:00, 172.59it/s, loss=0.599, acc=0.901]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 08: Train Loss: 0.1442 Acc: 0.9488 | Val Loss: 0.5994 Acc: 0.9010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 09/30: 100%|██████████| 1157/1157 [00:17<00:00, 65.31it/s, loss=0.12, acc=0.956]\n",
      "[Val] Epoch: 09/30: 100%|██████████| 16/16 [00:00<00:00, 171.70it/s, loss=0.621, acc=0.906]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 09: Train Loss: 0.1197 Acc: 0.9556 | Val Loss: 0.6206 Acc: 0.9060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 10/30: 100%|██████████| 1157/1157 [00:17<00:00, 65.25it/s, loss=0.104, acc=0.961]\n",
      "[Val] Epoch: 10/30: 100%|██████████| 16/16 [00:00<00:00, 172.04it/s, loss=0.754, acc=0.911]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10: Train Loss: 0.1035 Acc: 0.9610 | Val Loss: 0.7538 Acc: 0.9110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 11/30: 100%|██████████| 1157/1157 [00:17<00:00, 65.34it/s, loss=0.0954, acc=0.963]\n",
      "[Val] Epoch: 11/30: 100%|██████████| 16/16 [00:00<00:00, 164.73it/s, loss=0.703, acc=0.905]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11: Train Loss: 0.0954 Acc: 0.9631 | Val Loss: 0.7030 Acc: 0.9050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 12/30: 100%|██████████| 1157/1157 [00:17<00:00, 65.49it/s, loss=0.0872, acc=0.966]\n",
      "[Val] Epoch: 12/30: 100%|██████████| 16/16 [00:00<00:00, 162.99it/s, loss=0.665, acc=0.895]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12: Train Loss: 0.0872 Acc: 0.9664 | Val Loss: 0.6652 Acc: 0.8950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 13/30: 100%|██████████| 1157/1157 [00:17<00:00, 65.29it/s, loss=0.0788, acc=0.969]\n",
      "[Val] Epoch: 13/30: 100%|██████████| 16/16 [00:00<00:00, 167.41it/s, loss=0.818, acc=0.904]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13: Train Loss: 0.0788 Acc: 0.9688 | Val Loss: 0.8176 Acc: 0.9040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 14/30: 100%|██████████| 1157/1157 [00:17<00:00, 65.31it/s, loss=0.0737, acc=0.97]\n",
      "[Val] Epoch: 14/30: 100%|██████████| 16/16 [00:00<00:00, 170.06it/s, loss=0.79, acc=0.906]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14: Train Loss: 0.0737 Acc: 0.9703 | Val Loss: 0.7900 Acc: 0.9060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 15/30: 100%|██████████| 1157/1157 [00:17<00:00, 65.20it/s, loss=0.0574, acc=0.975]\n",
      "[Val] Epoch: 15/30: 100%|██████████| 16/16 [00:00<00:00, 163.55it/s, loss=0.855, acc=0.919]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15: Train Loss: 0.0574 Acc: 0.9749 | Val Loss: 0.8545 Acc: 0.9190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 16/30: 100%|██████████| 1157/1157 [00:17<00:00, 65.32it/s, loss=0.0496, acc=0.978]\n",
      "[Val] Epoch: 16/30: 100%|██████████| 16/16 [00:00<00:00, 165.85it/s, loss=0.931, acc=0.908]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16: Train Loss: 0.0496 Acc: 0.9782 | Val Loss: 0.9310 Acc: 0.9080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 17/30: 100%|██████████| 1157/1157 [00:17<00:00, 65.36it/s, loss=0.0488, acc=0.978]\n",
      "[Val] Epoch: 17/30: 100%|██████████| 16/16 [00:00<00:00, 169.28it/s, loss=1.11, acc=0.909]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17: Train Loss: 0.0488 Acc: 0.9779 | Val Loss: 1.1088 Acc: 0.9090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 18/30: 100%|██████████| 1157/1157 [00:17<00:00, 65.22it/s, loss=0.0487, acc=0.978]\n",
      "[Val] Epoch: 18/30: 100%|██████████| 16/16 [00:00<00:00, 169.70it/s, loss=1.02, acc=0.911]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18: Train Loss: 0.0487 Acc: 0.9778 | Val Loss: 1.0221 Acc: 0.9110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 19/30: 100%|██████████| 1157/1157 [00:17<00:00, 65.30it/s, loss=0.0496, acc=0.978]\n",
      "[Val] Epoch: 19/30: 100%|██████████| 16/16 [00:00<00:00, 165.34it/s, loss=0.951, acc=0.908]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19: Train Loss: 0.0496 Acc: 0.9776 | Val Loss: 0.9514 Acc: 0.9080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 20/30: 100%|██████████| 1157/1157 [00:17<00:00, 65.21it/s, loss=0.0444, acc=0.979]\n",
      "[Val] Epoch: 20/30: 100%|██████████| 16/16 [00:00<00:00, 164.96it/s, loss=1, acc=0.909]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20: Train Loss: 0.0444 Acc: 0.9789 | Val Loss: 1.0020 Acc: 0.9090\n",
      "Early stopping at epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "for epoch in range(1, epochs+1):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loop = tqdm(train_loader, desc=f\"[Train] Epoch: {epoch:02d}/{epochs}\")\n",
    "    total_loss, total, correct = 0.0, 0, 0\n",
    "    \n",
    "    for text, label in train_loop:\n",
    "        text, label = text.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(text)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * text.size(0)\n",
    "        preds = output.argmax(dim=1)\n",
    "        correct += (preds == label).sum().item()\n",
    "        total += text.size(0)\n",
    "        \n",
    "        train_loop.set_postfix({\n",
    "            'loss': total_loss / total,\n",
    "            'acc': correct / total\n",
    "        })\n",
    "\n",
    "    train_loss = total_loss / total\n",
    "    train_acc = correct / total\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loop = tqdm(val_loader, desc=f\"[Val] Epoch: {epoch:02d}/{epochs}\")\n",
    "    val_loss, val_total, val_correct = 0.0, 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text, label in val_loop:\n",
    "            text, label = text.to(device), label.to(device)\n",
    "            output = model(text)\n",
    "            loss = criterion(output, label)\n",
    "            \n",
    "            val_loss += loss.item() * text.size(0)\n",
    "            preds = output.argmax(dim=1)\n",
    "            val_correct += (preds == label).sum().item()\n",
    "            val_total += text.size(0)\n",
    "            \n",
    "            val_loop.set_postfix({\n",
    "                'loss': val_loss / val_total,\n",
    "                'acc': val_correct / val_total\n",
    "            })\n",
    "    \n",
    "    val_loss /= val_total\n",
    "    val_acc = val_correct / val_total\n",
    "    \n",
    "    scheduler.step(val_acc)  # Update learning rate\n",
    "    \n",
    "    # Print metrics every epoch\n",
    "    print(f\"\\nEpoch {epoch:02d}: \"\n",
    "          f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Early stopping check\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        early_stop_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ad6b5a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T13:41:32.737339Z",
     "iopub.status.busy": "2025-05-08T13:41:32.737054Z",
     "iopub.status.idle": "2025-05-08T13:41:32.779174Z",
     "shell.execute_reply": "2025-05-08T13:41:32.778343Z"
    },
    "papermill": {
     "duration": 1.185147,
     "end_time": "2025-05-08T13:41:32.780344",
     "exception": false,
     "start_time": "2025-05-08T13:41:31.595197",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: im getting on borderlands and i will murder you all\n",
      "Prediction: Positive\n",
      "Confidence: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def inference(sentence, max_seq=64, return_dict=False):\n",
    "    cleaned_text = clean_text(sentence)\n",
    "    if not cleaned_text.strip():\n",
    "        print(\"Input text is empty after cleaning\")\n",
    "        return None\n",
    "    tokens = tokenizer.tokenize(cleaned_text)\n",
    "    sequence = tokenizer.prepare_inputs(tokens, max_seq)\n",
    "    \n",
    "    input_tensor = torch.tensor(sequence).unsqueeze(0).to(device)\n",
    "    class_names = ['Positive', 'Neutral', 'Negative', 'Irrelevant']\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        outputs = model(input_tensor)\n",
    "        probabilities = F.softmax(outputs, dim=1) \n",
    "        conf, pred = torch.max(probabilities, dim=1)\n",
    "        \n",
    "        confidence = conf.item()\n",
    "        predicted_class = class_names[pred.item()]\n",
    "        \n",
    "        print(f\"Text: {sentence}\")\n",
    "        print(f\"Prediction: {predicted_class}\")\n",
    "        print(f\"Confidence: {confidence*100:.2f}%\")\n",
    "\n",
    "sentence = \"im getting on borderlands and i will murder you all\"\n",
    "result = inference(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba5b17af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T13:41:35.065433Z",
     "iopub.status.busy": "2025-05-08T13:41:35.065167Z",
     "iopub.status.idle": "2025-05-08T13:41:35.074373Z",
     "shell.execute_reply": "2025-05-08T13:41:35.073413Z"
    },
    "papermill": {
     "duration": 1.085371,
     "end_time": "2025-05-08T13:41:35.076020",
     "exception": false,
     "start_time": "2025-05-08T13:41:33.990649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: All these news are bad\n",
      "Prediction: Negative\n",
      "Confidence: 99.98%\n"
     ]
    }
   ],
   "source": [
    "sentence = \"All these news are bad\"\n",
    "result = inference(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45f47a78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T13:41:37.447557Z",
     "iopub.status.busy": "2025-05-08T13:41:37.447046Z",
     "iopub.status.idle": "2025-05-08T13:41:37.454689Z",
     "shell.execute_reply": "2025-05-08T13:41:37.453868Z"
    },
    "papermill": {
     "duration": 1.099256,
     "end_time": "2025-05-08T13:41:37.455922",
     "exception": false,
     "start_time": "2025-05-08T13:41:36.356666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: rock-hard la varlope, rare & powerful, handsome jackpot, borderlands 3 (xbox) dlvr.it/rmtrgf\n",
      "Prediction: Neutral\n",
      "Confidence: 100.00%\n"
     ]
    }
   ],
   "source": [
    "sentence = df_train.loc[12, 'clean_text']\n",
    "inference(sentence)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1520310,
     "sourceId": 2510329,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 441.536063,
   "end_time": "2025-05-08T13:41:41.567383",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-08T13:34:20.031320",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
