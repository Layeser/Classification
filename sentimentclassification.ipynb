{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0a592a1",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-08T13:48:53.026020Z",
     "iopub.status.busy": "2025-05-08T13:48:53.025763Z",
     "iopub.status.idle": "2025-05-08T13:49:36.633412Z",
     "shell.execute_reply": "2025-05-08T13:49:36.632583Z"
    },
    "papermill": {
     "duration": 43.617031,
     "end_time": "2025-05-08T13:49:36.637776",
     "exception": false,
     "start_time": "2025-05-08T13:48:53.020745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno -3] Temporary\n",
      "[nltk_data]     failure in name resolution>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "nltk.download('punkt')\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import re\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ff77823",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T13:49:36.648015Z",
     "iopub.status.busy": "2025-05-08T13:49:36.647624Z",
     "iopub.status.idle": "2025-05-08T13:49:36.658227Z",
     "shell.execute_reply": "2025-05-08T13:49:36.657599Z"
    },
    "papermill": {
     "duration": 0.015563,
     "end_time": "2025-05-08T13:49:36.659287",
     "exception": false,
     "start_time": "2025-05-08T13:49:36.643724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, corpus, min_freq=2):\n",
    "        self.corpus = corpus\n",
    "        self.tokens = nltk.word_tokenize(corpus.lower())\n",
    "        \n",
    "        self.freq = Counter(self.tokens)\n",
    "        \n",
    "        self.vocab = {\n",
    "            '[PAD]': 0,\n",
    "            '[UNK]': 1,\n",
    "            '[CLS]': 2,\n",
    "            '[SEP]': 3,\n",
    "        }\n",
    "        \n",
    "        idx = 4\n",
    "        for word in sorted(self.freq):\n",
    "            if self.freq[word] >= min_freq:\n",
    "                self.vocab[word] = idx\n",
    "                idx += 1\n",
    "\n",
    "        self.id_to_word = {idx: word for word, idx in self.vocab.items()}\n",
    "    \n",
    "    def tokenize(self, sentence):\n",
    "        return nltk.word_tokenize(sentence.lower())\n",
    "    \n",
    "    def batch_tokenize(self, batch_sentence):\n",
    "        return [self.tokenize(sentence) for sentence in tqdm(batch_sentence)]\n",
    "    \n",
    "    def encode(self, tokens):\n",
    "        return [self.vocab.get(word, self.vocab['[UNK]']) for word in tokens]\n",
    "    \n",
    "    def batch_encode(self, batch_tokens):\n",
    "        return [self.encode(tokens) for tokens in tqdm(batch_tokens)]\n",
    "    \n",
    "    def decode(self, sequence):\n",
    "        return ' '.join(self.id_to_word.get(idx, '[UNK]') for idx in sequence)\n",
    "    \n",
    "    def batch_decode(self, batch_sequences):\n",
    "        return [self.decode(seq) for seq in tqdm(batch_sequences)]\n",
    "    \n",
    "    def prepare_inputs(self, tokens, max_seq):\n",
    "        cls_token = [self.vocab['[CLS]']]\n",
    "        sep_token = [self.vocab['[SEP]']]\n",
    "        \n",
    "        encoded = self.encode(tokens)\n",
    "        max_tokens = max_seq - 2\n",
    "        truncated = encoded[:max_tokens]\n",
    "        \n",
    "        full_seq = cls_token + truncated + sep_token\n",
    "        padding_length = max_seq - len(full_seq)\n",
    "        padded_seq = full_seq + [self.vocab['[PAD]']] * padding_length\n",
    "        \n",
    "        return padded_seq\n",
    "    \n",
    "    def batch_prepare_inputs(self, batch_tokens, max_seq):\n",
    "        return [self.prepare_inputs(tokens, max_seq) for tokens in tqdm(batch_tokens)]\n",
    "\n",
    "    def decode_on_inputs(self, sequence, skip_special_tokens=True):\n",
    "        words = []\n",
    "        for idx in sequence:\n",
    "            word = self.id_to_word.get(idx, '[UNK]')\n",
    "            if skip_special_tokens and word in {'[PAD]', '[CLS]', '[SEP]'}:\n",
    "                continue\n",
    "            words.append(word)\n",
    "        return ' '.join(words)\n",
    "\n",
    "    def batch_decode_on_inputs(self, batch_sequences, skip_special_tokens=True):\n",
    "        return [self.decode_on_inputs(seq, skip_special_tokens) for seq in tqdm(batch_sequences)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab09cd3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T13:49:36.666554Z",
     "iopub.status.busy": "2025-05-08T13:49:36.666345Z",
     "iopub.status.idle": "2025-05-08T13:49:36.670474Z",
     "shell.execute_reply": "2025-05-08T13:49:36.669806Z"
    },
    "papermill": {
     "duration": 0.009177,
     "end_time": "2025-05-08T13:49:36.671819",
     "exception": false,
     "start_time": "2025-05-08T13:49:36.662642",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    df = df = pd.read_csv(path, encoding='latin-1', header=None).drop([0, 1], axis=1)\n",
    "    df.columns = ['sentiment', 'text']\n",
    "    sentiment2id = {'Positive': 0, 'Neutral': 1, 'Negative': 2, 'Irrelevant': 3}\n",
    "    df['sentiment'] = df['sentiment'].map(sentiment2id)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65cf074d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T13:49:36.678982Z",
     "iopub.status.busy": "2025-05-08T13:49:36.678774Z",
     "iopub.status.idle": "2025-05-08T13:49:36.683529Z",
     "shell.execute_reply": "2025-05-08T13:49:36.682838Z"
    },
    "papermill": {
     "duration": 0.009707,
     "end_time": "2025-05-08T13:49:36.684634",
     "exception": false,
     "start_time": "2025-05-08T13:49:36.674927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean raw text for tokenization:\n",
    "      - Lowercase all text\n",
    "      - Remove URLs (http, https, www) and t.co short links\n",
    "      - Collapse long runs of punctuation\n",
    "      - Strip out non‑ASCII (e.g. emojis)\n",
    "      - Remove stray slashes and repeated slashes\n",
    "      - Normalize whitespace\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Remove URLs (http://, https://, www.) and Twitter t.co links\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+|//t\\.co/\\S+', ' ', text)\n",
    "\n",
    "    # 3. Collapse runs of dots (4+ → '...') and other punctuation (4+ → single char)\n",
    "    text = re.sub(r'\\.{4,}', '...', text)\n",
    "    text = re.sub(r'([!?])\\1{1,}', r'\\1', text)\n",
    "\n",
    "    # 4. Remove stray slashes (2+ → single '/')\n",
    "    text = re.sub(r'/{2,}', '/', text)\n",
    "\n",
    "    # 5. Strip non‑ASCII (emojis, fancy quotes, etc.)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "\n",
    "    # 6. Remove any remaining standalone punctuation tokens longer than 3 chars\n",
    "    text = re.sub(r'([^\\w\\s]){4,}', r'\\1', text)\n",
    "\n",
    "    # 7. Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf4030d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T13:49:36.692172Z",
     "iopub.status.busy": "2025-05-08T13:49:36.691958Z",
     "iopub.status.idle": "2025-05-08T13:49:36.976775Z",
     "shell.execute_reply": "2025-05-08T13:49:36.976200Z"
    },
    "papermill": {
     "duration": 0.289664,
     "end_time": "2025-05-08T13:49:36.978102",
     "exception": false,
     "start_time": "2025-05-08T13:49:36.688438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_train = \"/kaggle/input/twitter-entity-sentiment-analysis/twitter_training.csv\"\n",
    "path_val = \"/kaggle/input/twitter-entity-sentiment-analysis/twitter_validation.csv\"\n",
    "\n",
    "df_train = load_data(path_train)\n",
    "df_val = load_data(path_val)\n",
    "df_train = df_train.dropna(subset=['text'])\n",
    "df_val = df_val.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d724962",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T13:49:36.985309Z",
     "iopub.status.busy": "2025-05-08T13:49:36.985080Z",
     "iopub.status.idle": "2025-05-08T13:49:38.724972Z",
     "shell.execute_reply": "2025-05-08T13:49:38.724407Z"
    },
    "papermill": {
     "duration": 1.744799,
     "end_time": "2025-05-08T13:49:38.726251",
     "exception": false,
     "start_time": "2025-05-08T13:49:36.981452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train['clean_text'] = df_train['text'].apply(clean_text)\n",
    "df_val['clean_text'] = df_val['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6322feb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T13:49:38.733433Z",
     "iopub.status.busy": "2025-05-08T13:49:38.733211Z",
     "iopub.status.idle": "2025-05-08T13:49:38.740087Z",
     "shell.execute_reply": "2025-05-08T13:49:38.739515Z"
    },
    "papermill": {
     "duration": 0.011537,
     "end_time": "2025-05-08T13:49:38.741114",
     "exception": false,
     "start_time": "2025-05-08T13:49:38.729577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('im getting on borderlands and i will murder you all ,', 0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['clean_text'][0], df_train['sentiment'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e6db964",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T13:49:38.748080Z",
     "iopub.status.busy": "2025-05-08T13:49:38.747864Z",
     "iopub.status.idle": "2025-05-08T13:49:48.512796Z",
     "shell.execute_reply": "2025-05-08T13:49:48.512201Z"
    },
    "papermill": {
     "duration": 9.769839,
     "end_time": "2025-05-08T13:49:48.514188",
     "exception": false,
     "start_time": "2025-05-08T13:49:38.744349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "texts = \" \".join(df_train['text'].values)\n",
    "corpus = clean_text(texts)\n",
    "tokenizer = Tokenizer(\" \".join(df_train['clean_text'].values), min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2031cadf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T13:49:48.521694Z",
     "iopub.status.busy": "2025-05-08T13:49:48.521448Z",
     "iopub.status.idle": "2025-05-08T13:49:48.525862Z",
     "shell.execute_reply": "2025-05-08T13:49:48.525307Z"
    },
    "papermill": {
     "duration": 0.009327,
     "end_time": "2025-05-08T13:49:48.526984",
     "exception": false,
     "start_time": "2025-05-08T13:49:48.517657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28406"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd5e80df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T13:49:48.534311Z",
     "iopub.status.busy": "2025-05-08T13:49:48.534115Z",
     "iopub.status.idle": "2025-05-08T13:49:59.918898Z",
     "shell.execute_reply": "2025-05-08T13:49:59.918078Z"
    },
    "papermill": {
     "duration": 11.389757,
     "end_time": "2025-05-08T13:49:59.920180",
     "exception": false,
     "start_time": "2025-05-08T13:49:48.530423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73996/73996 [00:10<00:00, 7070.36it/s]\n",
      "100%|██████████| 73996/73996 [00:00<00:00, 104487.60it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 7047.26it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 124401.00it/s]\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = 64\n",
    "train_inputs = tokenizer.batch_prepare_inputs(\n",
    "    tokenizer.batch_tokenize(df_train['clean_text'].values), max_seq=max_seq_len\n",
    ")\n",
    "val_inputs = tokenizer.batch_prepare_inputs(\n",
    "    tokenizer.batch_tokenize(df_val['clean_text'].values), max_seq=max_seq_len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2b78fdf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T13:49:59.936299Z",
     "iopub.status.busy": "2025-05-08T13:49:59.936090Z",
     "iopub.status.idle": "2025-05-08T13:50:00.754149Z",
     "shell.execute_reply": "2025-05-08T13:50:00.753526Z"
    },
    "papermill": {
     "duration": 0.827435,
     "end_time": "2025-05-08T13:50:00.755445",
     "exception": false,
     "start_time": "2025-05-08T13:49:59.928010",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_inputs = torch.tensor(train_inputs)\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "train_labels = torch.tensor(df_train['sentiment'].values, dtype=torch.long)\n",
    "val_labels = torch.tensor(df_val['sentiment'].values, dtype=torch.long)\n",
    "\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_labels)\n",
    "val_data = TensorDataset(val_inputs, val_labels)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58bcdf4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T13:50:00.771788Z",
     "iopub.status.busy": "2025-05-08T13:50:00.771533Z",
     "iopub.status.idle": "2025-05-08T13:50:00.804470Z",
     "shell.execute_reply": "2025-05-08T13:50:00.803843Z"
    },
    "papermill": {
     "duration": 0.042144,
     "end_time": "2025-05-08T13:50:00.805560",
     "exception": false,
     "start_time": "2025-05-08T13:50:00.763416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: [CLS] i mentioned on facebook that i was struggling for motivation to go for a run the other day , which has been translated by tom s great auntie as hayley can t get out of bed and told to his grandma , who now thinks i m a lazy , terrible person [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Label: 3\n"
     ]
    }
   ],
   "source": [
    "for text, label in val_loader:\n",
    "    print(f\"Text: {tokenizer.decode(text[0].tolist())}\\nLabel: {label[0]}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93120a33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T13:50:00.821610Z",
     "iopub.status.busy": "2025-05-08T13:50:00.821406Z",
     "iopub.status.idle": "2025-05-08T13:50:00.826242Z",
     "shell.execute_reply": "2025-05-08T13:50:00.825717Z"
    },
    "papermill": {
     "duration": 0.013722,
     "end_time": "2025-05-08T13:50:00.827238",
     "exception": false,
     "start_time": "2025-05-08T13:50:00.813516",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: i mentioned on facebook that i was struggling for motivation to go for a run the other day , which has been translated by tom s great auntie as hayley can t get out of bed and told to his grandma , who now thinks i m a lazy , terrible person\n",
      "Label: 3\n"
     ]
    }
   ],
   "source": [
    "for text, label in val_loader:\n",
    "    print(f\"Text: {tokenizer.decode_on_inputs(text[0].tolist())}\\nLabel: {label[0]}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d82be95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T13:50:00.842564Z",
     "iopub.status.busy": "2025-05-08T13:50:00.842367Z",
     "iopub.status.idle": "2025-05-08T13:50:01.191610Z",
     "shell.execute_reply": "2025-05-08T13:50:01.191027Z"
    },
    "papermill": {
     "duration": 0.35835,
     "end_time": "2025-05-08T13:50:01.192971",
     "exception": false,
     "start_time": "2025-05-08T13:50:00.834621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SentimentAnalysis(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, num_classes, dropout_rate=0.5):\n",
    "        super(SentimentAnalysis, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, \n",
    "                            batch_first=True, dropout=0.2 if num_layers > 1 else 0.)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "        hidden_last = hidden[-1] \n",
    "        hidden_last = self.dropout(hidden_last)\n",
    "        out = self.fc(hidden_last)\n",
    "        return out\n",
    "\n",
    "\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SentimentAnalysis(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=128,\n",
    "    hidden_dim=256,\n",
    "    num_layers=2,\n",
    "    num_classes=4,\n",
    "    dropout_rate=0.5\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f1566ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T13:50:01.209279Z",
     "iopub.status.busy": "2025-05-08T13:50:01.208615Z",
     "iopub.status.idle": "2025-05-08T13:50:01.214851Z",
     "shell.execute_reply": "2025-05-08T13:50:01.214162Z"
    },
    "papermill": {
     "duration": 0.015244,
     "end_time": "2025-05-08T13:50:01.215959",
     "exception": false,
     "start_time": "2025-05-08T13:50:01.200715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=3, factor=0.5)  # Added\n",
    "\n",
    "best_val_acc = 0\n",
    "early_stop_counter = 0\n",
    "patience = 5  # Added early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2c738b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T13:50:01.231760Z",
     "iopub.status.busy": "2025-05-08T13:50:01.231526Z",
     "iopub.status.idle": "2025-05-08T13:58:16.892266Z",
     "shell.execute_reply": "2025-05-08T13:58:16.891597Z"
    },
    "papermill": {
     "duration": 495.790754,
     "end_time": "2025-05-08T13:58:17.014086",
     "exception": false,
     "start_time": "2025-05-08T13:50:01.223332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 01/30: 100%|██████████| 1157/1157 [00:18<00:00, 62.61it/s, loss=1.37, acc=0.296]\n",
      "[Val] Epoch: 01/30: 100%|██████████| 16/16 [00:00<00:00, 171.17it/s, loss=1.37, acc=0.266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01: Train Loss: 1.3702 Acc: 0.2957 | Val Loss: 1.3742 Acc: 0.2660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 02/30: 100%|██████████| 1157/1157 [00:17<00:00, 65.93it/s, loss=1.37, acc=0.301]\n",
      "[Val] Epoch: 02/30: 100%|██████████| 16/16 [00:00<00:00, 170.37it/s, loss=1.38, acc=0.266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 02: Train Loss: 1.3677 Acc: 0.3005 | Val Loss: 1.3757 Acc: 0.2660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 03/30: 100%|██████████| 1157/1157 [00:17<00:00, 66.00it/s, loss=1.37, acc=0.301]\n",
      "[Val] Epoch: 03/30: 100%|██████████| 16/16 [00:00<00:00, 172.32it/s, loss=1.37, acc=0.266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 03: Train Loss: 1.3671 Acc: 0.3008 | Val Loss: 1.3742 Acc: 0.2660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 04/30: 100%|██████████| 1157/1157 [00:17<00:00, 65.89it/s, loss=1.36, acc=0.311]\n",
      "[Val] Epoch: 04/30: 100%|██████████| 16/16 [00:00<00:00, 170.62it/s, loss=1.26, acc=0.411]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 04: Train Loss: 1.3611 Acc: 0.3107 | Val Loss: 1.2591 Acc: 0.4110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 05/30: 100%|██████████| 1157/1157 [00:17<00:00, 65.82it/s, loss=1.14, acc=0.472]\n",
      "[Val] Epoch: 05/30: 100%|██████████| 16/16 [00:00<00:00, 161.78it/s, loss=0.975, acc=0.526]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 05: Train Loss: 1.1416 Acc: 0.4722 | Val Loss: 0.9754 Acc: 0.5260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 06/30: 100%|██████████| 1157/1157 [00:17<00:00, 65.92it/s, loss=0.882, acc=0.623]\n",
      "[Val] Epoch: 06/30: 100%|██████████| 16/16 [00:00<00:00, 166.97it/s, loss=0.587, acc=0.794]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 06: Train Loss: 0.8817 Acc: 0.6233 | Val Loss: 0.5873 Acc: 0.7940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 07/30: 100%|██████████| 1157/1157 [00:17<00:00, 65.93it/s, loss=0.569, acc=0.786]\n",
      "[Val] Epoch: 07/30: 100%|██████████| 16/16 [00:00<00:00, 172.29it/s, loss=0.394, acc=0.865]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 07: Train Loss: 0.5694 Acc: 0.7857 | Val Loss: 0.3938 Acc: 0.8650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 08/30: 100%|██████████| 1157/1157 [00:17<00:00, 65.90it/s, loss=0.368, acc=0.869]\n",
      "[Val] Epoch: 08/30: 100%|██████████| 16/16 [00:00<00:00, 173.91it/s, loss=0.305, acc=0.902]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 08: Train Loss: 0.3677 Acc: 0.8690 | Val Loss: 0.3051 Acc: 0.9020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 09/30: 100%|██████████| 1157/1157 [00:17<00:00, 66.06it/s, loss=0.256, acc=0.912]\n",
      "[Val] Epoch: 09/30: 100%|██████████| 16/16 [00:00<00:00, 166.30it/s, loss=0.236, acc=0.93]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 09: Train Loss: 0.2560 Acc: 0.9117 | Val Loss: 0.2363 Acc: 0.9300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 10/30: 100%|██████████| 1157/1157 [00:17<00:00, 65.83it/s, loss=0.188, acc=0.935]\n",
      "[Val] Epoch: 10/30: 100%|██████████| 16/16 [00:00<00:00, 166.47it/s, loss=0.229, acc=0.94]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10: Train Loss: 0.1879 Acc: 0.9350 | Val Loss: 0.2290 Acc: 0.9400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 11/30: 100%|██████████| 1157/1157 [00:17<00:00, 65.92it/s, loss=0.152, acc=0.948]\n",
      "[Val] Epoch: 11/30: 100%|██████████| 16/16 [00:00<00:00, 171.40it/s, loss=0.269, acc=0.942]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11: Train Loss: 0.1517 Acc: 0.9477 | Val Loss: 0.2688 Acc: 0.9420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 12/30: 100%|██████████| 1157/1157 [00:17<00:00, 65.97it/s, loss=0.123, acc=0.956]\n",
      "[Val] Epoch: 12/30: 100%|██████████| 16/16 [00:00<00:00, 163.53it/s, loss=0.238, acc=0.945]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12: Train Loss: 0.1234 Acc: 0.9558 | Val Loss: 0.2384 Acc: 0.9450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 13/30: 100%|██████████| 1157/1157 [00:17<00:00, 66.04it/s, loss=0.106, acc=0.962]\n",
      "[Val] Epoch: 13/30: 100%|██████████| 16/16 [00:00<00:00, 170.83it/s, loss=0.255, acc=0.95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13: Train Loss: 0.1062 Acc: 0.9618 | Val Loss: 0.2550 Acc: 0.9500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 14/30: 100%|██████████| 1157/1157 [00:17<00:00, 65.97it/s, loss=0.0942, acc=0.965]\n",
      "[Val] Epoch: 14/30: 100%|██████████| 16/16 [00:00<00:00, 168.73it/s, loss=0.297, acc=0.951]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14: Train Loss: 0.0942 Acc: 0.9648 | Val Loss: 0.2966 Acc: 0.9510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 15/30: 100%|██████████| 1157/1157 [00:17<00:00, 65.86it/s, loss=0.0857, acc=0.968]\n",
      "[Val] Epoch: 15/30: 100%|██████████| 16/16 [00:00<00:00, 168.40it/s, loss=0.263, acc=0.952]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15: Train Loss: 0.0857 Acc: 0.9676 | Val Loss: 0.2634 Acc: 0.9520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 16/30: 100%|██████████| 1157/1157 [00:17<00:00, 65.93it/s, loss=0.0796, acc=0.97]\n",
      "[Val] Epoch: 16/30: 100%|██████████| 16/16 [00:00<00:00, 166.95it/s, loss=0.234, acc=0.95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16: Train Loss: 0.0796 Acc: 0.9697 | Val Loss: 0.2339 Acc: 0.9500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 17/30: 100%|██████████| 1157/1157 [00:17<00:00, 65.86it/s, loss=0.0765, acc=0.971]\n",
      "[Val] Epoch: 17/30: 100%|██████████| 16/16 [00:00<00:00, 168.55it/s, loss=0.287, acc=0.953]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17: Train Loss: 0.0765 Acc: 0.9706 | Val Loss: 0.2873 Acc: 0.9530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 18/30: 100%|██████████| 1157/1157 [00:17<00:00, 65.95it/s, loss=0.0717, acc=0.972]\n",
      "[Val] Epoch: 18/30: 100%|██████████| 16/16 [00:00<00:00, 154.57it/s, loss=0.223, acc=0.959]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18: Train Loss: 0.0717 Acc: 0.9716 | Val Loss: 0.2231 Acc: 0.9590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 19/30: 100%|██████████| 1157/1157 [00:17<00:00, 65.91it/s, loss=0.0679, acc=0.972]\n",
      "[Val] Epoch: 19/30: 100%|██████████| 16/16 [00:00<00:00, 169.84it/s, loss=0.25, acc=0.952]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19: Train Loss: 0.0679 Acc: 0.9725 | Val Loss: 0.2504 Acc: 0.9520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 20/30: 100%|██████████| 1157/1157 [00:17<00:00, 65.88it/s, loss=0.0656, acc=0.973]\n",
      "[Val] Epoch: 20/30: 100%|██████████| 16/16 [00:00<00:00, 167.89it/s, loss=0.218, acc=0.953]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20: Train Loss: 0.0656 Acc: 0.9734 | Val Loss: 0.2177 Acc: 0.9530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 21/30: 100%|██████████| 1157/1157 [00:17<00:00, 65.93it/s, loss=0.0638, acc=0.974]\n",
      "[Val] Epoch: 21/30: 100%|██████████| 16/16 [00:00<00:00, 167.99it/s, loss=0.202, acc=0.958]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21: Train Loss: 0.0638 Acc: 0.9739 | Val Loss: 0.2019 Acc: 0.9580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 22/30: 100%|██████████| 1157/1157 [00:17<00:00, 65.95it/s, loss=0.0617, acc=0.974]\n",
      "[Val] Epoch: 22/30: 100%|██████████| 16/16 [00:00<00:00, 169.17it/s, loss=0.228, acc=0.956]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22: Train Loss: 0.0617 Acc: 0.9745 | Val Loss: 0.2282 Acc: 0.9560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 23/30: 100%|██████████| 1157/1157 [00:17<00:00, 66.00it/s, loss=0.0503, acc=0.977]\n",
      "[Val] Epoch: 23/30: 100%|██████████| 16/16 [00:00<00:00, 167.13it/s, loss=0.224, acc=0.967]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23: Train Loss: 0.0503 Acc: 0.9775 | Val Loss: 0.2239 Acc: 0.9670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 24/30: 100%|██████████| 1157/1157 [00:17<00:00, 65.99it/s, loss=0.0444, acc=0.98]\n",
      "[Val] Epoch: 24/30: 100%|██████████| 16/16 [00:00<00:00, 167.87it/s, loss=0.254, acc=0.963]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24: Train Loss: 0.0444 Acc: 0.9795 | Val Loss: 0.2538 Acc: 0.9630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 25/30: 100%|██████████| 1157/1157 [00:17<00:00, 65.99it/s, loss=0.0443, acc=0.98]\n",
      "[Val] Epoch: 25/30: 100%|██████████| 16/16 [00:00<00:00, 170.16it/s, loss=0.277, acc=0.96]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25: Train Loss: 0.0443 Acc: 0.9796 | Val Loss: 0.2773 Acc: 0.9600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 26/30: 100%|██████████| 1157/1157 [00:17<00:00, 65.86it/s, loss=0.0464, acc=0.979]\n",
      "[Val] Epoch: 26/30: 100%|██████████| 16/16 [00:00<00:00, 168.79it/s, loss=0.263, acc=0.957]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26: Train Loss: 0.0464 Acc: 0.9787 | Val Loss: 0.2630 Acc: 0.9570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 27/30: 100%|██████████| 1157/1157 [00:17<00:00, 66.21it/s, loss=0.046, acc=0.98]\n",
      "[Val] Epoch: 27/30: 100%|██████████| 16/16 [00:00<00:00, 163.04it/s, loss=0.265, acc=0.956]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27: Train Loss: 0.0460 Acc: 0.9796 | Val Loss: 0.2653 Acc: 0.9560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 28/30: 100%|██████████| 1157/1157 [00:17<00:00, 65.89it/s, loss=0.0424, acc=0.98]\n",
      "[Val] Epoch: 28/30: 100%|██████████| 16/16 [00:00<00:00, 169.38it/s, loss=0.291, acc=0.957]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28: Train Loss: 0.0424 Acc: 0.9801 | Val Loss: 0.2912 Acc: 0.9570\n",
      "Early stopping at epoch 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "for epoch in range(1, epochs+1):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loop = tqdm(train_loader, desc=f\"[Train] Epoch: {epoch:02d}/{epochs}\")\n",
    "    total_loss, total, correct = 0.0, 0, 0\n",
    "    \n",
    "    for text, label in train_loop:\n",
    "        text, label = text.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(text)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * text.size(0)\n",
    "        preds = output.argmax(dim=1)\n",
    "        correct += (preds == label).sum().item()\n",
    "        total += text.size(0)\n",
    "        \n",
    "        train_loop.set_postfix({\n",
    "            'loss': total_loss / total,\n",
    "            'acc': correct / total\n",
    "        })\n",
    "\n",
    "    train_loss = total_loss / total\n",
    "    train_acc = correct / total\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loop = tqdm(val_loader, desc=f\"[Val] Epoch: {epoch:02d}/{epochs}\")\n",
    "    val_loss, val_total, val_correct = 0.0, 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text, label in val_loop:\n",
    "            text, label = text.to(device), label.to(device)\n",
    "            output = model(text)\n",
    "            loss = criterion(output, label)\n",
    "            \n",
    "            val_loss += loss.item() * text.size(0)\n",
    "            preds = output.argmax(dim=1)\n",
    "            val_correct += (preds == label).sum().item()\n",
    "            val_total += text.size(0)\n",
    "            \n",
    "            val_loop.set_postfix({\n",
    "                'loss': val_loss / val_total,\n",
    "                'acc': val_correct / val_total\n",
    "            })\n",
    "    \n",
    "    val_loss /= val_total\n",
    "    val_acc = val_correct / val_total\n",
    "    \n",
    "    scheduler.step(val_acc)  # Update learning rate\n",
    "    \n",
    "    # Print metrics every epoch\n",
    "    print(f\"\\nEpoch {epoch:02d}: \"\n",
    "          f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Early stopping check\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        early_stop_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f0a57dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T13:58:20.238393Z",
     "iopub.status.busy": "2025-05-08T13:58:20.237532Z",
     "iopub.status.idle": "2025-05-08T13:58:20.243532Z",
     "shell.execute_reply": "2025-05-08T13:58:20.242869Z"
    },
    "papermill": {
     "duration": 1.580709,
     "end_time": "2025-05-08T13:58:20.244633",
     "exception": false,
     "start_time": "2025-05-08T13:58:18.663924",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inference(sentence, max_seq=64, return_dict=False):\n",
    "    cleaned_text = clean_text(sentence)\n",
    "    if not cleaned_text.strip():\n",
    "        print(\"Input text is empty after cleaning\")\n",
    "        return None\n",
    "    tokens = tokenizer.tokenize(cleaned_text)\n",
    "    sequence = tokenizer.prepare_inputs(tokens, max_seq)\n",
    "    \n",
    "    input_tensor = torch.tensor(sequence).unsqueeze(0).to(device)\n",
    "    class_names = ['Positive', 'Neutral', 'Negative', 'Irrelevant']\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        outputs = model(input_tensor)\n",
    "        probabilities = F.softmax(outputs, dim=1) \n",
    "        conf, pred = torch.max(probabilities, dim=1)\n",
    "        \n",
    "        confidence = conf.item()\n",
    "        predicted_class = class_names[pred.item()]\n",
    "        \n",
    "        print(f\"Text: {sentence}\")\n",
    "        print(f\"Prediction: {predicted_class}\")\n",
    "        print(f\"Confidence: {confidence*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d552c271",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T13:58:23.390033Z",
     "iopub.status.busy": "2025-05-08T13:58:23.389469Z",
     "iopub.status.idle": "2025-05-08T13:58:23.431341Z",
     "shell.execute_reply": "2025-05-08T13:58:23.430664Z"
    },
    "papermill": {
     "duration": 1.609874,
     "end_time": "2025-05-08T13:58:23.432599",
     "exception": false,
     "start_time": "2025-05-08T13:58:21.822725",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: All these news are bad\n",
      "Prediction: Negative\n",
      "Confidence: 100.00%\n"
     ]
    }
   ],
   "source": [
    "sentence = \"All these news are bad\"\n",
    "result = inference(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "295e1ef0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T13:58:26.545625Z",
     "iopub.status.busy": "2025-05-08T13:58:26.545350Z",
     "iopub.status.idle": "2025-05-08T13:58:26.552705Z",
     "shell.execute_reply": "2025-05-08T13:58:26.551906Z"
    },
    "papermill": {
     "duration": 1.551971,
     "end_time": "2025-05-08T13:58:26.553926",
     "exception": false,
     "start_time": "2025-05-08T13:58:25.001955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: rock-hard la varlope, rare & powerful, handsome jackpot, borderlands 3 (xbox) dlvr.it/rmtrgf\n",
      "Prediction: Neutral\n",
      "Confidence: 100.00%\n"
     ]
    }
   ],
   "source": [
    "sentence = df_train.loc[12, 'clean_text']\n",
    "inference(sentence)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1520310,
     "sourceId": 2510329,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 581.848301,
   "end_time": "2025-05-08T13:58:30.800068",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-08T13:48:48.951767",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
